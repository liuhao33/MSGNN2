{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess\n",
    "This section is adopted from MAGNN(https://github.com/cynricfu/MAGNN).\n",
    "\n",
    "Before running, you should:\n",
    "\n",
    "1. Visit MAGNN to download the required glove.6B.50d.txt file \n",
    "   \n",
    "    And visit HAN(https://github.com/Jhy1993/HAN) to download DBLP4057_GAT_with_idx.mat. \n",
    "\n",
    "    Save two files to `./data`.\n",
    "\n",
    "2. Install nltk and download 4 nltk data(see below).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.set_proxy('http://127.0.0.1:1080')\n",
    "#nltk.download_gui()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import networkx as nx \n",
    "import utils.preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from utils.data import load_glove_vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_prefix = r'./data/DBLP_processed/'\n",
    "read_perfix = r'./data/DBLP/'\n",
    "num_ntypes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "author_label = pd.read_csv(read_perfix + 'author_label.txt', sep='\\t', header=None, names=['author_id', 'label', 'author_name'], keep_default_na=False, encoding='utf-8')\n",
    "paper_author = pd.read_csv(read_perfix + 'paper_author.txt', sep='\\t', header=None, names=['paper_id', 'author_id'], keep_default_na=False, encoding='utf-8')\n",
    "paper_conf = pd.read_csv(read_perfix + 'paper_conf.txt', sep='\\t', header=None, names=['paper_id', 'conf_id'], keep_default_na=False, encoding='utf-8')\n",
    "paper_term = pd.read_csv(read_perfix + 'paper_term.txt', sep='\\t', header=None, names=['paper_id', 'term_id'], keep_default_na=False, encoding='utf-8')\n",
    "papers = pd.read_csv(read_perfix + 'paper.txt', sep='\\t', header=None, names=['paper_id', 'paper_title'], keep_default_na=False, encoding='cp1252')\n",
    "terms = pd.read_csv(read_perfix + 'term.txt', sep='\\t', header=None, names=['term_id', 'term'], keep_default_na=False, encoding='utf-8')\n",
    "confs = pd.read_csv(read_perfix + 'conf.txt', sep='\\t', header=None, names=['conf_id', 'conf'], keep_default_na=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe pretrained word vectors\n",
      "Done. 400000 words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove_dim = 50\n",
    "glove_vectors = load_glove_vectors(dim=glove_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# filter out all nodes which does not associated with labeled authors\n",
    "labeled_authors = author_label['author_id'].to_list()\n",
    "paper_author = paper_author[paper_author['author_id'].isin(labeled_authors)].reset_index(drop=True)\n",
    "valid_papers = paper_author['paper_id'].unique()\n",
    "papers = papers[papers['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_conf = paper_conf[paper_conf['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_term = paper_term[paper_term['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "valid_terms = paper_term['term_id'].unique()\n",
    "terms = terms[terms['term_id'].isin(valid_terms)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      term_id         term        lemma  lemma_id\n",
      "0           1          the          the         1\n",
      "1           2    automatic    automatic         2\n",
      "2           3  acquisition  acquisition         3\n",
      "3           4           of           of         4\n",
      "4           5        proof        proof         5\n",
      "...       ...          ...          ...       ...\n",
      "8893    13567      gapprox      gapprox     13567\n",
      "8894    13568       poetry       poetry     13568\n",
      "8895    13569       estmax       estmax     13569\n",
      "8896    13570        zonal        zonal     13570\n",
      "8897    13571    fractures     fracture     13571\n",
      "\n",
      "[8898 rows x 4 columns]\n",
      "        paper_id  term_id\n",
      "0           7601        7\n",
      "1           7601        8\n",
      "2           7601        9\n",
      "3           7601       10\n",
      "4           7601       11\n",
      "...          ...      ...\n",
      "114268    654269      580\n",
      "114269    654269      723\n",
      "114270    654269      902\n",
      "114271    654269     1653\n",
      "114272    654269     2229\n",
      "\n",
      "[114273 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# term lemmatization and grouping\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_id_mapping = {}\n",
    "lemma_list = []\n",
    "lemma_id_list = []\n",
    "i = 0\n",
    "for _, row in terms.iterrows():\n",
    "    i += 1\n",
    "    lemma = lemmatizer.lemmatize(row['term'])\n",
    "    lemma_list.append(lemma)\n",
    "    if lemma not in lemma_id_mapping:\n",
    "        lemma_id_mapping[lemma] = row['term_id']\n",
    "    lemma_id_list.append(lemma_id_mapping[lemma])\n",
    "terms['lemma'] = lemma_list\n",
    "terms['lemma_id'] = lemma_id_list\n",
    "\n",
    "print(terms)\n",
    "print(paper_term)\n",
    "\n",
    "term_lemma_mapping = {row['term_id']: row['lemma_id'] for _, row in terms.iterrows()}\n",
    "lemma_id_list = []\n",
    "for _, row in paper_term.iterrows():\n",
    "    lemma_id_list.append(term_lemma_mapping[row['term_id']])\n",
    "paper_term['lemma_id'] = lemma_id_list\n",
    "\n",
    "paper_term = paper_term[['paper_id', 'lemma_id']]\n",
    "paper_term.columns = ['paper_id', 'term_id']\n",
    "paper_term = paper_term.drop_duplicates()\n",
    "terms = terms[['lemma_id', 'lemma']]\n",
    "terms.columns = ['term_id', 'term']\n",
    "terms = terms.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# filter out stopwords from terms\n",
    "stopwords = sklearn_stopwords.union(set(nltk_stopwords.words('english')))\n",
    "stopword_id_list = terms[terms['term'].isin(stopwords)]['term_id'].to_list()\n",
    "paper_term = paper_term[~(paper_term['term_id'].isin(stopword_id_list))].reset_index(drop=True)\n",
    "terms = terms[~(terms['term'].isin(stopwords))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "author_label = author_label.sort_values('author_id').reset_index(drop=True)\n",
    "papers = papers.sort_values('paper_id').reset_index(drop=True)\n",
    "terms = terms.sort_values('term_id').reset_index(drop=True)\n",
    "confs = confs.sort_values('conf_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 3 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# extract labels of authors\n",
    "labels = author_label['label'].to_numpy()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# build the adjacency matrix for the graph consisting of authors, papers, terms and conferences\n",
    "# 0 for authors, 1 for papers, 2 for terms, 3 for conferences\n",
    "dim = len(author_label) + len(papers) + len(terms) + len(confs)\n",
    "type_mask = np.zeros((dim), dtype=int)\n",
    "type_mask[len(author_label):len(author_label)+len(papers)] = 1\n",
    "type_mask[len(author_label)+len(papers):len(author_label)+len(papers)+len(terms)] = 2\n",
    "type_mask[len(author_label)+len(papers)+len(terms):] = 3\n",
    "\n",
    "author_id_mapping = {row['author_id']: i for i, row in author_label.iterrows()}\n",
    "paper_id_mapping = {row['paper_id']: i + len(author_label) for i, row in papers.iterrows()}\n",
    "term_id_mapping = {row['term_id']: i + len(author_label) + len(papers) for i, row in terms.iterrows()}\n",
    "conf_id_mapping = {row['conf_id']: i + len(author_label) + len(papers) + len(terms) for i, row in confs.iterrows()}\n",
    "\n",
    "adjM = np.zeros((dim, dim), dtype=int)\n",
    "for _, row in paper_author.iterrows(): \n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = author_id_mapping[row['author_id']]\n",
    "    adjM[idx1, idx2] = 1\n",
    "    adjM[idx2, idx1] = 1\n",
    "for _, row in paper_term.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = term_id_mapping[row['term_id']]\n",
    "    adjM[idx1, idx2] = 1\n",
    "    adjM[idx2, idx1] = 1\n",
    "for _, row in paper_conf.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = conf_id_mapping[row['conf_id']]\n",
    "    adjM[idx1, idx2] = 1\n",
    "    adjM[idx2, idx1] = 1\n",
    "print(adjM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use HAN paper's preprocessed data as the features of authors (https://github.com/Jhy1993/HAN)\n",
    "mat = scipy.io.loadmat('data/DBLP4057_GAT_with_idx.mat')\n",
    "features_author = np.array(list(zip(*sorted(zip(labeled_authors, mat['features']), key=lambda tup: tup[0])))[1])\n",
    "features_author = scipy.sparse.csr_matrix(features_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\ohnn\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'doe', 'ha', 'le', \"n't\", 'need', 'sha', 'u', 'wa', 'wo'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    }
   ],
   "source": [
    "# use bag-of-words representation of paper titles as the features of papers\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "vectorizer = CountVectorizer(min_df=2, stop_words=stopwords, tokenizer=LemmaTokenizer())\n",
    "features_paper = vectorizer.fit_transform(papers['paper_title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use pretrained GloVe vectors as the features of terms \n",
    "features_term = np.zeros((len(terms), glove_dim))\n",
    "for i, row in terms.iterrows():\n",
    "    features_term[i] = glove_vectors.get(row['term'], glove_vectors['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author_label.to_csv(save_prefix + 'author_label.csv')\n",
    "# papers.to_csv(save_prefix + 'papers.csv')\n",
    "# terms.to_csv(save_prefix + 'terms.csv')\n",
    "# confs.to_csv(save_prefix + 'confs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Schema instances\n",
    "\n",
    "We search schema instances based on the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = {0: {0: {'main': [0, 1, 3]}}, 1: {0: {'main': [1, 2], 'to': 0}}}  # output of Schema decomposition\n",
    "\n",
    "schema = utils.preprocess.plan_transform_v2(plan)\n",
    "schema\n",
    "# schema = {'stem': [0, 1, 3], 'branch': {0: [1, 2]}}  # or decompose manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare some operators (adj, type_mask, prefix_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4057, 14328, 7723, 20]\n",
      "26128\n",
      "[    0  4057 18385 26108 26128]\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "adj = scipy.sparse.lil_matrix(adjM)\n",
    "raw_dims = [sum(type_mask==p) for p in range(num_ntypes)] # num of each type\n",
    "dim = sum(raw_dims) # total num of nodes\n",
    "print(raw_dims)\n",
    "print(dim)\n",
    "prefix_operator = np.ones((len(raw_dims)+1, len(raw_dims)))\n",
    "prefix_operator = np.tril(prefix_operator, k=-1)   \n",
    "prefix_operator = prefix_operator.dot(raw_dims).astype(int)\n",
    "print(prefix_operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_intances = utils.preprocess.get_intances(adj, type_mask, schema, prefix_operator)\n",
    "#print(chain_intances)\n",
    "subgraphs = utils.preprocess.get_schema_subgraphs_parallel(schema, chain_intances)\n",
    "subgraphs = subgraphs[subgraphs.columns.sort_values()]\n",
    "print('=======done=======')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraphs = subgraphs.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3030669656076822\n",
      "0.09650154405854731\n",
      "0.6004314903337704\n"
     ]
    }
   ],
   "source": [
    "# subgraphs train/validation/test splits\n",
    "rand_seed = 33333333\n",
    "train_val_idx, test_idx = train_test_split(np.arange(len(adjM)), test_size=0.1, random_state=rand_seed)\n",
    "a = np.isin(subgraphs,test_idx)\n",
    "a = a.sum(axis=1).astype('bool')\n",
    "subgraphs_test = subgraphs[a]\n",
    "subgraphs_tr_val = subgraphs[~a]\n",
    "subgraphs[a].shape\n",
    "print(subgraphs_test.shape[0]/len(subgraphs)) # 30% for test\n",
    "train_idx, val_idx = train_test_split(train_val_idx, test_size=0.025, random_state=rand_seed)\n",
    "b = np.isin(subgraphs_tr_val,val_idx)\n",
    "b = b.sum(axis=1).astype('bool')\n",
    "subgraphs_val = subgraphs_tr_val[b]\n",
    "subgraphs_train = subgraphs_tr_val[~b]\n",
    "subgraphs_tr_val[b].shape\n",
    "print(subgraphs_val.shape[0]/len(subgraphs)) # 10% for val\n",
    "print(len(subgraphs_train)/len(subgraphs)) # 60% for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(save_prefix + 'subgraphs_train_val_test.npz',\n",
    "         subgraphs_train=subgraphs_train,\n",
    "         subgraphs_val=subgraphs_val,\n",
    "         subgraphs_test=subgraphs_test) # subgraph train&val&test\n",
    "# save data\n",
    "np.save(save_prefix + 'schema.npy', schema) # schema\n",
    "# type prefix\n",
    "np.save(save_prefix + 'prefix_operator.npy',prefix_operator)\n",
    "# subgraph table\n",
    "np.save(save_prefix + 'subgraphs.npy', subgraphs)\n",
    "# all nodes adjacency matrix\n",
    "scipy.sparse.save_npz(save_prefix + 'adjM.npz', scipy.sparse.csr_matrix(adjM))\n",
    "# all nodes (authors, papers, terms and conferences) features\n",
    "# currently only have features of authors, papers and terms\n",
    "scipy.sparse.save_npz(save_prefix + 'features_{}.npz'.format(0), features_author)\n",
    "scipy.sparse.save_npz(save_prefix + 'features_{}.npz'.format(1), features_paper)\n",
    "np.save(save_prefix + 'features_{}.npy'.format(2), features_term)\n",
    "# all nodes (authors, papers, terms and conferences) type labels\n",
    "np.save(save_prefix + 'node_types.npy', type_mask)\n",
    "# author labels\n",
    "np.save(save_prefix + 'labels.npy', labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('other')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5df5fa93f361bdbb0b7071e5a3f6d23ca72a33a86af435fe3cd957604527782b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
